# Липшицевость функций

Когда мы говорим о непрерывных функциях, важно помнить, что непрерывность сама по себе не накладывает ограничений на скорость изменения функции. Функция может быть непрерывной, но при этом резко меняться, что может вызвать сложности в задачах оптимизации и анализа. Именно здесь на помощь приходит понятие Липшицева непрерывность, которое вводит более строгий контроль за тем, насколько сильно значения функции могут изменяться в ответ на изменения входных данных.

 

**Липшицева непрерывность**

Функция $f(x)$ называется непрерывной по Липшицу на множестве $D$, если существует константа $L>0$, такая что для любых $x_1, x_2 \in D$ выполняется:

$|f(x_1) - f(x_2)| \leq L |x_1 - x_2|.$

Липшицева непрерывность — более строгая форма непрерывности, которая ограничивает скорость изменения функции. Это свойство важно для анализа сходимости и устойчивости алгоритмов.

Что нам дает непрерывность по Липшицу?

- **Гарантия сходимости:** Липшицева непрерывность градиента функции потерь позволяет определить шаг обучения в градиентном спуске для достижения стабильной и быстрой сходимости.


- **Устойчивость к шуму:** Модели с функциями, удовлетворяющими условию Липшица, менее чувствительны к выбросам в данных.

  
  **Влияние непрерывности по Липшицу на сходимость градиентного спуска**

  Функция $f(x)$ имеет непрерывный градиент по Липшицу на множестве $D$, если существует константа $L > 0$, такая что для любых $x_1, x_2 \in D$ выполняется:

  $\|\nabla f(x_1) - \nabla f(x_2)\| \leq L \|x_1 - x_2\|    (1).  $

  Здесь $\nabla f(x)$ — градиент функции в точке $x$, а $\|\cdot\|$ обозначает евклидову норму.

  
  
  ---

  **Утверждение:** Для любых $x, y$ выполняется для Липшицевой функции выполняется:

  $$
  f(y) \leq f(x) + \nabla f(x)^\top (y - x) + \dfrac{L}{2} \|y - x\|^2   (2).
  $$

  
   **Доказательство:**

  **(1) → (2):**

  Предположим, что выполняется условие 1. Тогда разложим $f(y)$ через формулу Тейлора:

   $f(y) = f(x) + \langle \nabla f(x), y - x \rangle + R(x, y)$, 

  где $R(x,y$) — остаточный член. Для функции с L-Липшицевым градиентом остаточный член можно ограничить сверху:

   $R(x, y) \leq \frac{L}{2} \|y - x\|^2$. 

  Таким образом, получается:

   $f(y) \leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \|y - x\|^2$. 

  **(2) → (1):**

  Теперь предположим, что выполняется условие 2. Проверим, что градиент удовлетворяет условию Липшица: Рассмотрим $\phi(t) = f(x + t (y - x))$, где $t \in [0, 1]$. Тогда:

   $\phi'(t) = \langle \nabla f(x + t(y - x)), y - x \rangle.$ 

  Из условия 2 для $f(x)$ следует:

   $\phi(1) \leq \phi(0) + \phi'(0) + \frac{L}{2} \|y - x\|^2$. 

  Подставляя, видим, что при $t \to 1$ разница в градиентах ограничена L-Липшицевым постоянным:

   $\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|$. 

  ---

   

  Таким образом, оба условия эквивалентны.
- 
  **Теорема:** Пусть функция $f(x)$ выпукла и имеет непрерывный градиент по Липшицу с константой $L$. Тогда при выборе шага обучения $0 < \alpha \leq \dfrac{1}{L}$ последовательность ${x_k}$, определяемая правилом градиентного спуска $x_{k+1} = x_k - \alpha \nabla f(x_k)$, сходится к минимуму функции $f(x)$.

  **Доказательство:**

  Используем свойство непрерывности градиента по Липшицу:

  Для любых $x, y$ выполняется для Липшицевой функции выполняется:

  $$
  f(y) \leq f(x) + \nabla f(x)^\top (y - x) + \dfrac{L}{2} \|y - x\|^2.
  $$

  Подставим $y = x_{k+1} = x_k - \alpha \nabla f(x_k)$:

  $$
  f(x_{k+1}) \leq f(x_k) - \alpha \|\nabla f(x_k)\|^2 + \dfrac{L \alpha^2}{2} \|\nabla f(x_k)\|^2.  
  $$

  Здесь мы использовали, что $y - x = -\alpha \nabla f(x_k)$.

  Упростим выражение: $f(x_{k+1}) \leq f(x_k) - \left( \alpha - \dfrac{L \alpha^2}{2} \right) \|\nabla f(x_k)\|^2.$

  Выберем шаг обучения $\alpha \leq \dfrac{1}{L}$:

  Тогда $\alpha - \dfrac{L \alpha^2}{2} \geq \dfrac{\alpha}{2}$.

  Действительно:

  $$
  \alpha - \dfrac{L \alpha^2}{2} \geq \alpha - \dfrac{\alpha^2}{2} \geq \dfrac{\alpha}{2} \quad \text{при} \ L \geq 1.
  $$

  Получаем оценку убывания функции потерь:

  $$
   f(x_{k+1}) \leq f(x_k) - \dfrac{\alpha}{2} \|\nabla f(x_k)\|^2.
  $$

  Следовательно:
-  функция $f(x)$ убывает на каждом шаге градиентного спуска
- а сумма $\sum_{k=0}^\infty \|\nabla f(x_k)\|^2 < c(f(x_k)-f(x_{k+1}))$ сходится, значит, $||\nabla f(x_k)||\to 0$. Таким образом, алгоритм градиентного спуска сходится.


**Выводы:** 

1) нужно держать в голове, что для стабильной сходимости градиентного спуска нам недостаточно просто дифференцируемой функции потерь. Например, если взять не квадратичную ошибку (MSE), а четвертую степень ошибки - у нас теоретически уже могут возникнуть проблемы со сходимостью градиентного спуска из-за того, что мы не понимаем, в каком диапазоне находится $\alpha$ для сходимости. Возможно, нам придется использовать адаптивные методы (AdaGrad, AdaDelta, Adam и так далее), чтобы градиентный спуск для нелипшицевой функции сошелся.

2) Необязательна липшицевость на всей области определения функции. Часто достаточно, чтобы градиент функции потерь был непрерывен по липшицу на некотором компакте, на котором предположительно находится минимум.


[Источник мудрости - 1.](http://www.machinelearning.ru/wiki/images/archive/3/34/20140423115800!Rodomanov-fast-gradient-methods.pdf)

[Источник мудрости - 2.](https://education.yandex.ru/handbook/ml/article/shodimost-sgd) 


Пример: <https://colab.research.google.com/drive/1FXtvaFBiwbLwNIz84NGXf1DeAw2s3y1u?usp=sharing> 


*Материал подготовлен по мотивам обсуждений и главы в Хендбуке по математике для анализа данных авторства Альбины Бурловой (студентки 2 курса магистратуры “Искусственный интеллект“).*
